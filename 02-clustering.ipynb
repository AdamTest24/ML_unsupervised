{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ff4df124",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Clustering\"\n",
    "teaching: 10\n",
    "exercises: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef961b",
   "metadata": {},
   "source": [
    "[**Download Chapter pdf**](02-clustering.md.pdf)\n",
    "\n",
    "[**Download Chapter notebook (ipynb)**](02-clustering.ipynb)\n",
    "\n",
    "[<span style=\"color: rgb(255, 0, 0);\">**Mandatory Lesson Feedback Survey**</span>](https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b6b96",
   "metadata": {},
   "source": [
    "- \n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fd6cf",
   "metadata": {},
   "source": [
    "- Working with a specialised Python package\n",
    "- Clustering with Gaussian mixture models (GMM) to segment different parts of a medical image\n",
    "- Combining information from different imaging modalities for improved segmentation\n",
    "- Strategies to visualise clustering output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfd95c",
   "metadata": {},
   "source": [
    "- [Data Handling Images]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81239b32",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "### **Image Segmentation with Clustering**\n",
    "\n",
    "Medical imaging techniques are a valuable tool for probing diseases and disorders non-invasively. Processing medical images often consists of an expert, such as a radiologist, looking at the images and identifying, labelling and characterising potential lesions. This process can be time-consuming and reliant on a well-trained expert's eye. To make medical imaging techniques feasible in circumstances where there may me insufficient time or resources for expert labelling, there is current research into using artificial intelligence to label images. There are many supervised learning techniques that utilise previously labelled data from experts to train a computer algorithm to recognise certain features of an image. However, this may require large amounts of data that were previously labelled, which again is not always available. An alternative approach is to use unsupervised learning strategies, such as clustering, to group images into different regions. The interpretation of these regions may be ambiguous, but with some previous knowledge, we may still use it to infer information from an image.\n",
    "\n",
    "### **Medical Image Example**\n",
    "\n",
    "The example used in this lesson is part of the National Cancer Instituteâ€™s Clinical Proteomic Tumor Analysis Consortium Glioblastoma Multiforme (CPTAC-GBM) cohort and is available at The Cancer Imaging Archive: https://wiki.cancerimagingarchive.net/display/Public/CPTAC-GBM. For each subject in this study, several different brain MRI scans were performed, each of which gives different contrast in the brain. Each subject has been diagnosed with glioblastoma, and a tumour is visible in the MRI scans. To analyse the images and to, for example, estimate the size of the tumour, we may wish to segment the brain into healthy tissue and tumour tissue. The figure below shows four images in the different modalities.\n",
    "\n",
    "![Figure 1: Example image, different MRI modalities imaging glioblastoma](fig/brain_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33379ca",
   "metadata": {},
   "source": [
    "## Work Through Example\n",
    "\n",
    "### **Code Preparation**\n",
    "\n",
    "We first import the modules needed for this lesson. We use Numpy to store and process images and we use __nibabel__ to read the MRI images, which have a file type called 'nifti'. Nibabel is freely available for download here: https://nipy.org/nibabel/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, sum, stack\n",
    "import nibabel as nib\n",
    "from matplotlib.pyplot import subplots, tight_layout, show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26363d",
   "metadata": {},
   "source": [
    "## Note\n",
    "Note how we import the nibabel package as 'nib'. You can use any abbreviation to access the package's functions from within your programme. \n",
    "\n",
    "To familiarise yourself with the nibabel package, try the [Getting started tutorial](https://nipy.org/nibabel/gettingstarted.html) using an example image file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f22b2",
   "metadata": {},
   "source": [
    "### **Reading Images into Numpy Arrays**\n",
    "\n",
    "Next, we want to use the nibabel package to read the MRI images into Numpy arrays. In this example, we use four different images that were acquired with different MRI protocols.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_3d = nib.load('fig/t1.nii')\n",
    "img1 = img_3d.get_fdata()\n",
    "\n",
    "img_3d = nib.load('fig/t1_contrast.nii')\n",
    "img2 = img_3d.get_fdata()\n",
    "\n",
    "img_3d = nib.load('fig/flair.nii')\n",
    "img3 = img_3d.get_fdata()\n",
    "\n",
    "img_3d = nib.load('fig/adc.nii')\n",
    "img4 = img_3d.get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a79d5",
   "metadata": {},
   "source": [
    "Let's have a look at the data shape:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac321fd",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(256, 256, 32)\n",
    "```\n",
    "\n",
    "For plotting, we select a slice from the images. In this example we will view axial slices, i.e. slices from the last dimension. Thus, we choose a slice number between 0 and 31, here we go with slice 20 and plot it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f974d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "img_slice = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb678062",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f020802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=1, ncols=4, figsize=(20, 10))\n",
    "\n",
    "ax[0].imshow(img1[:, :, img_slice], cmap='gray')\n",
    "ax[0].set_title(\"T1\", fontsize=16)\n",
    "ax[1].imshow(img2[:, :, img_slice], cmap='gray')\n",
    "ax[1].set_title(\"T1 with contrast agent\", fontsize=16)\n",
    "ax[2].imshow(img3[:, :, img_slice], cmap='gray')\n",
    "ax[2].set_title(\"FLAIR\", fontsize=16)\n",
    "ax[3].imshow(img4[:, :, img_slice], cmap='gray')\n",
    "ax[3].set_title(\"Apparent diffusion coefficient\", fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da877b",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-5-1.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "### **Data Pre-processing**\n",
    "\n",
    "To analyse the images, we need to do a bit of preprocessing. First of all, lets plot the histogram of the voxel (volume pixel) intensities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=1, ncols=4, figsize=(20, 4))\n",
    "\n",
    "ax[0].hist(img1.flatten(), bins=50);\n",
    "ax[0].set_title(\"T1\", fontsize=16)\n",
    "ax[0].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[1].hist(img2.flatten(), bins=50);\n",
    "ax[1].set_title(\"T1 with contrast agent\", fontsize=16)\n",
    "ax[1].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[2].hist(img3.flatten(), bins=50);\n",
    "ax[2].set_title(\"FLAIR\", fontsize=16)\n",
    "ax[2].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[3].hist(img4.flatten(), bins=50);\n",
    "ax[3].set_title(\"Apparent diffusion coefficient\", fontsize=16)\n",
    "ax[3].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "print('Number of voxels with intensity equal to 0 is: %d'%sum(img1==0))\n",
    "print('')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb36f56",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "Number of voxels with intensity equal to 0 is: 1848804\n",
    "```\n",
    "\n",
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-6-3.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "As we can see from these histograms, a large number of the values are zero. This corresponds to the background voxels shown in black. We want to remove these voxels, as they are not useful for our analysis. For this, we create a binary mask and apply it to the images.\n",
    "\n",
    "## Note\n",
    "Note the use of `tight_layout` in the cell above. It is a [Matplotlib function](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.tight_layout.html) to pad between the figure edge and the edges of subplots. This can be useful to avoid overlap of figures and labels. The keyword parameter `pad` is set to 1.08 by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5edc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (img1>0) & (img2>0) & (img3>0) & (img4>0)\n",
    "\n",
    "img1_nz = img1[mask]\n",
    "img2_nz = img2[mask]\n",
    "img3_nz = img3[mask]\n",
    "img4_nz = img4[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54880e71",
   "metadata": {},
   "source": [
    "With the mask applied, let us plot the histograms of the non-zero voxels again:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "ax[0].hist(img1_nz, bins=50);\n",
    "ax[0].set_title(\"T1\", fontsize=16)\n",
    "ax[0].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[1].hist(img2_nz, bins=50);\n",
    "ax[1].set_title(\"T1 with contrast agent\", fontsize=16)\n",
    "ax[1].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[2].hist(img3_nz, bins=50);\n",
    "ax[2].set_title(\"FLAIR\", fontsize=16)\n",
    "ax[2].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[3].hist(img4_nz, bins=50);\n",
    "ax[3].set_title(\"Apparent diffusion coefficient\", fontsize=16)\n",
    "ax[3].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9bd56",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-8-5.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "We can see that the data is no longer confounded by the zero-valued background voxels. The distribution of relevant intensities now becomes apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddb35f",
   "metadata": {},
   "source": [
    "### **Image scaling**\n",
    "\n",
    "In many machine learning applications (both supervised and unsupervised) an additional step of data preparation consists in normalising or scaling, i.e. adjustment of the values under certain conditions. For example, the numbers in a data file are all positive and very large but the algorithms work best for numbers with mean zero and variance 1. In scikit-learn this can be done by using `fit_transform` for an instance of the `StandardScaler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "img1_scaled = scaler.fit_transform(img1_nz.reshape(-1, 1))\n",
    "img2_scaled = scaler.fit_transform(img2_nz.reshape(-1, 1))\n",
    "img3_scaled = scaler.fit_transform(img3_nz.reshape(-1, 1))\n",
    "img4_scaled = scaler.fit_transform(img4_nz.reshape(-1, 1))\n",
    "\n",
    "fig, ax = subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "ax[0].hist(img1_scaled, bins=50);\n",
    "ax[0].set_title(\"T1\", fontsize=16)\n",
    "ax[0].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[1].hist(img2_scaled, bins=50);\n",
    "ax[1].set_title(\"T1 with contrast agent\", fontsize=16)\n",
    "ax[1].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[2].hist(img3_scaled, bins=50);\n",
    "ax[2].set_title(\"FLAIR\", fontsize=16)\n",
    "ax[2].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "ax[3].hist(img4_scaled, bins=50);\n",
    "ax[3].set_title(\"Apparent diffusion coefficient\", fontsize=16)\n",
    "ax[3].set_xlabel(\"Intensity\", fontsize=16)\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197f6a4",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-9-7.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "If you compare the histograms, you can see that the values in the data have changed (horizontal axis) but the shapes of the distributions are the same.\n",
    "\n",
    "We are not persuing this further here but you are encouraged to re-do the clustering below with the scaled images and check if there are any differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6dc9a",
   "metadata": {},
   "source": [
    "### **Image Segmentation with Clustering**\n",
    "\n",
    "After this data cleaning, we can proceed with our analysis. We want to use the images to segment the images into brain tissue and tumour tissue. It is not obvious how to do this, as the intensity values in the above histogram are continuous with only one major peak in intensity. We will nonetheless attempt to cluster the images using a Gaussian Mixture Model (GMM).\n",
    "\n",
    "First, we import the GMM class from scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac644348",
   "metadata": {},
   "source": [
    "We then fit the instantiated model with a few different numbers of clusters (argument `n_components`, between n = 2-4) individually for each image. We use the `fit_predict` method to simultaneously fit and label the images. Note that we also add 1 to each image label. This is because each data point gets labelled with a number from 0 to _n-1_ where _n_ is the number of clusters we use to fit the model. At the plotting stage, we do not want any of our labels to be equal to 0, as this corresponds to the background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cdb94",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "gmm_2 = GaussianMixture(2)\n",
    "img1_n2_labels = gmm_2.fit_predict(img1_nz.reshape(-1, 1))\n",
    "img1_n2_labels += 1\n",
    "\n",
    "gmm_3 = GaussianMixture(3)\n",
    "img1_n3_labels = gmm_3.fit_predict(img1_nz.reshape(-1, 1))\n",
    "img1_n3_labels += 1\n",
    "\n",
    "gmm_4 = GaussianMixture(4)\n",
    "img1_n4_labels = gmm_4.fit_predict(img1_nz.reshape(-1, 1))\n",
    "img1_n4_labels += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d490142",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c0017",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "gmm_2 = GaussianMixture(2)\n",
    "img2_n2_labels = gmm_2.fit_predict(img2_nz.reshape(-1, 1))\n",
    "img2_n2_labels += 1\n",
    "\n",
    "gmm_3 = GaussianMixture(3)\n",
    "img2_n3_labels = gmm_3.fit_predict(img2_nz.reshape(-1, 1))\n",
    "img2_n3_labels += 1\n",
    "\n",
    "gmm_4 = GaussianMixture(4)\n",
    "img2_n4_labels = gmm_4.fit_predict(img2_nz.reshape(-1, 1))\n",
    "img2_n4_labels += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97fb3d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be05f0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "gmm_2 = GaussianMixture(2)\n",
    "img3_n2_labels = gmm_2.fit_predict(img3_nz.reshape(-1, 1))\n",
    "img3_n2_labels += 1\n",
    "\n",
    "gmm_3 = GaussianMixture(3)\n",
    "img3_n3_labels = gmm_3.fit_predict(img3_nz.reshape(-1, 1))\n",
    "img3_n3_labels += 1\n",
    "\n",
    "gmm_4 = GaussianMixture(4)\n",
    "img3_n4_labels = gmm_4.fit_predict(img3_nz.reshape(-1, 1))\n",
    "img3_n4_labels += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5929a86",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_2 = GaussianMixture(2)\n",
    "img4_n2_labels = gmm_2.fit_predict(img4_nz.reshape(-1, 1))\n",
    "img4_n2_labels += 1\n",
    "\n",
    "gmm_3 = GaussianMixture(3)\n",
    "img4_n3_labels = gmm_3.fit_predict(img4_nz.reshape(-1, 1))\n",
    "img4_n3_labels += 1\n",
    "\n",
    "gmm_4 = GaussianMixture(4)\n",
    "img4_n4_labels = gmm_4.fit_predict(img4_nz.reshape(-1, 1))\n",
    "img4_n4_labels += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74920b6",
   "metadata": {},
   "source": [
    "Once we have all our image labels, we map the labels back to the three-dimensional image array and plot the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3defa0d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "img1_n2_labels_mapped = zeros(img1.shape)\n",
    "img1_n2_labels_mapped[mask] = img1_n2_labels\n",
    "\n",
    "img1_n3_labels_mapped = zeros(img1.shape)\n",
    "img1_n3_labels_mapped[mask] = img1_n3_labels\n",
    "\n",
    "img1_n4_labels_mapped = zeros(img1.shape)\n",
    "img1_n4_labels_mapped[mask] = img1_n4_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadd3db",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cad97",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "img2_n2_labels_mapped = zeros(img2.shape)\n",
    "img2_n2_labels_mapped[mask] = img2_n2_labels\n",
    "\n",
    "img2_n3_labels_mapped = zeros(img2.shape)\n",
    "img2_n3_labels_mapped[mask] = img2_n3_labels\n",
    "\n",
    "img2_n4_labels_mapped = zeros(img2.shape)\n",
    "img2_n4_labels_mapped[mask] = img2_n4_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f9b9c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3262d4d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "img3_n2_labels_mapped = zeros(img3.shape)\n",
    "img3_n2_labels_mapped[mask] = img3_n2_labels\n",
    "\n",
    "img3_n3_labels_mapped = zeros(img3.shape)\n",
    "img3_n3_labels_mapped[mask] = img3_n3_labels\n",
    "\n",
    "img3_n4_labels_mapped = zeros(img3.shape)\n",
    "img3_n4_labels_mapped[mask] = img3_n4_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bda4e3",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878e194",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "img4_n2_labels_mapped = zeros(img4.shape)\n",
    "img4_n2_labels_mapped[mask] = img4_n2_labels\n",
    "\n",
    "img4_n3_labels_mapped = zeros(img4.shape)\n",
    "img4_n3_labels_mapped[mask] = img4_n3_labels\n",
    "\n",
    "img4_n4_labels_mapped = zeros(img4.shape)\n",
    "img4_n4_labels_mapped[mask] = img4_n4_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2942342d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ff06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "ax[0, 0].imshow(img1_n2_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[1, 0].imshow(img1_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[2, 0].imshow(img1_n4_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[0, 1].imshow(img2_n2_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[1, 1].imshow(img2_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[2, 1].imshow(img2_n4_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "\n",
    "ax[0, 2].imshow(img3_n2_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[1, 2].imshow(img3_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[2, 2].imshow(img3_n4_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "\n",
    "ax[0, 3].imshow(img4_n2_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[1, 3].imshow(img4_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[2, 3].imshow(img4_n4_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "\n",
    "ax[0, 0].set_ylabel(\"2 clusters\", fontsize=18)\n",
    "ax[1, 0].set_ylabel(\"3 clusters\", fontsize=18)\n",
    "ax[2, 0].set_ylabel(\"4 clusters\", fontsize=18)\n",
    "\n",
    "ax[0, 0].set_title(\"Image 1\", fontsize=18)\n",
    "ax[0, 1].set_title(\"Image 2\", fontsize=18)\n",
    "ax[0, 2].set_title(\"Image 3\", fontsize=18)\n",
    "ax[0, 3].set_title(\"Image 4\", fontsize=18)\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04d9fb",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-19-9.png\" width=\"1920\" style=\"display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5b637",
   "metadata": {},
   "source": [
    "This figure shows the labels acquired from each of the images, using different numbers of clusters. We see that using Image 3 (acquired with FLAIR protocol), the lesion is segmented quite well from the rest of the brain. The other images are less effective at clearly identifying the lesion. However, some of these images, e.g. image 3 (apparent diffusion coefficient) performs better at segmenting brain tissue from surrounding cerebrospinal fluid (CSF). CSF is not part of brain tissue and can contaminate our results. Ideally, we want to segment three key areas: brain, lesion and CSF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800579a2",
   "metadata": {},
   "source": [
    "### **Combining Contrast from Different Images**\n",
    "\n",
    "So far, we only used the intensities of each image individually, i.e. using only one feature. We now try to combine the images into a single Numpy array containing four columns, one for each image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img = stack([img1_nz, img2_nz, img3_nz, img4_nz], axis=1)\n",
    "all_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae7e13",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(240391, 4)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef455895",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "gmm_3 = GaussianMixture(3)\n",
    "\n",
    "all_img_n3_labels = gmm_3.fit_predict(all_img)\n",
    "all_img_n3_labels += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbee60",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0709349",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "all_img_n3_labels_mapped = zeros(img1.shape)\n",
    "all_img_n3_labels_mapped[mask] = all_img_n3_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0becb792",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 5, figsize=(20, 5))\n",
    "ax[0].imshow(img1_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[1].imshow(img2_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[2].imshow(img3_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[3].imshow(img4_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "ax[4].imshow(all_img_n3_labels_mapped[:, :, img_slice], cmap='viridis')\n",
    "\n",
    "ax[0].set_ylabel(\"3 clusters\", fontsize=18)\n",
    "\n",
    "ax[0].set_title(\"Image 1\", fontsize=18)\n",
    "ax[1].set_title(\"Image 2\", fontsize=18)\n",
    "ax[2].set_title(\"Image 3\", fontsize=18)\n",
    "ax[3].set_title(\"Image 4\", fontsize=18)\n",
    "ax[4].set_title(\"All images\", fontsize=18)\n",
    "\n",
    "tight_layout()\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325ce58",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-23-11.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Here, the last column shows the cluster results when all four images are used in the Gaussian mixture model. These results seem to be better than the individual images, and with three clusters, the lesion, CSF and brain tissue seem clearly identifyable.\n",
    "\n",
    "Let's plot some of the other image slices to check that the segmentation performs well in the whole image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6276c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "ax[0, 0].imshow(img1[:, :, 16], cmap='gray')\n",
    "ax[0, 1].imshow(img1[:, :, 18], cmap='gray')\n",
    "ax[0, 2].imshow(img1[:, :, 20], cmap='gray')\n",
    "ax[0, 3].imshow(img1[:, :, 22], cmap='gray')\n",
    "ax[0, 4].imshow(img1[:, :, 24], cmap='gray')\n",
    "\n",
    "ax[1, 0].imshow(img2[:, :, 16], cmap='gray')\n",
    "ax[1, 1].imshow(img2[:, :, 18], cmap='gray')\n",
    "ax[1, 2].imshow(img2[:, :, 20], cmap='gray')\n",
    "ax[1, 3].imshow(img2[:, :, 22], cmap='gray')\n",
    "ax[1, 4].imshow(img2[:, :, 24], cmap='gray')\n",
    "\n",
    "ax[2, 0].imshow(img3[:, :, 16], cmap='gray')\n",
    "ax[2, 1].imshow(img3[:, :, 18], cmap='gray')\n",
    "ax[2, 2].imshow(img3[:, :, 20], cmap='gray')\n",
    "ax[2, 3].imshow(img3[:, :, 22], cmap='gray')\n",
    "ax[2, 4].imshow(img3[:, :, 24], cmap='gray')\n",
    "\n",
    "ax[3, 0].imshow(img4[:, :, 16], cmap='gray')\n",
    "ax[3, 1].imshow(img4[:, :, 18], cmap='gray')\n",
    "ax[3, 2].imshow(img4[:, :, 20], cmap='gray')\n",
    "ax[3, 3].imshow(img4[:, :, 22], cmap='gray')\n",
    "ax[3, 4].imshow(img4[:, :, 24], cmap='gray')\n",
    "\n",
    "ax[4, 0].imshow(all_img_n3_labels_mapped[:, :, 16], cmap='viridis')\n",
    "ax[4, 1].imshow(all_img_n3_labels_mapped[:, :, 18], cmap='viridis')\n",
    "ax[4, 2].imshow(all_img_n3_labels_mapped[:, :, 20], cmap='viridis')\n",
    "ax[4, 3].imshow(all_img_n3_labels_mapped[:, :, 22], cmap='viridis')\n",
    "ax[4, 4].imshow(all_img_n3_labels_mapped[:, :, 24], cmap='viridis')\n",
    "\n",
    "ax[0, 0].set_title(\"Slice 16\", fontsize=16)\n",
    "ax[0, 1].set_title(\"Slice 18\", fontsize=16)\n",
    "ax[0, 2].set_title(\"Slice 20\", fontsize=16)\n",
    "ax[0, 3].set_title(\"Slice 22\", fontsize=16)\n",
    "ax[0, 4].set_title(\"Slice 24\", fontsize=16)\n",
    "\n",
    "ax[0, 0].set_ylabel(\"Image 1\", fontsize=16)\n",
    "ax[1, 0].set_ylabel(\"Image 2\", fontsize=16)\n",
    "ax[2, 0].set_ylabel(\"Image 3\", fontsize=16)\n",
    "ax[3, 0].set_ylabel(\"Image 4\", fontsize=16)\n",
    "ax[4, 0].set_ylabel(\"Clustered labels\", fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eac6d",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-24-13.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Overall, the lesion, shown in yellow, seems to be segmented well across the volume.\n",
    "\n",
    "### **Checking the GMM Labels**\n",
    "\n",
    "To investigate how the image intensities were clustered, we can look at the scatter plots for each combination of images. The diagonal plots show histograms of each image. This type of plot can be very useful in exploratory data analysis. \n",
    "\n",
    "## Note\n",
    "Note that this plot might take a bit longer to run, as there are a very large number of data points. \n",
    "\n",
    "In a python plotting library called [seaborn](https://seaborn.pydata.org), such plots are called `pairplots` and can be very easily plotted if your data is in a pandas dataframe.\n",
    "\n",
    "You can install it at the command prompt (Windows) or terminal (MacOS, Linux) using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da473b",
   "metadata": {},
   "source": [
    "```\n",
    "conda install seaborn\n",
    "```\n",
    "\n",
    "To use it, import the required functions in your Python kernel, e.g.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4105f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import pairplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c736d3",
   "metadata": {},
   "source": [
    "```{.error}\n",
    "Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: cannot import name 'pairplots' from 'seaborn' (/home/runner/.virtualenvs/r-env/lib/python3.10/site-packages/seaborn/__init__.py)\n",
    "\n",
    "Detailed traceback:\n",
    "  File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "We don't use this library here, but encourage you to look up further information in [the seaborn documentation](https://seaborn.pydata.org/generated/seaborn.pairplot.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(4, 4, figsize=(20, 20))\n",
    "\n",
    "ax[0, 0].hist(img1_nz, bins=50);\n",
    "ax[0, 0].set_title('Image 1', fontsize=16)\n",
    "ax[0, 1].scatter(img1_nz, img2_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[0, 1].set_xlabel('Image 1', fontsize=16)\n",
    "ax[0, 1].set_ylabel('Image 2', fontsize=16)\n",
    "ax[0, 2].scatter(img1_nz, img3_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[0, 2].set_xlabel('Image 1', fontsize=16)\n",
    "ax[0, 2].set_ylabel('Image 3', fontsize=16)\n",
    "ax[0, 3].scatter(img1_nz, img4_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[0, 3].set_xlabel('Image 1', fontsize=16)\n",
    "ax[0, 3].set_ylabel('Image 4', fontsize=16)\n",
    "\n",
    "ax[1, 0].scatter(img2_nz, img1_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[1, 0].set_xlabel('Image 2', fontsize=16)\n",
    "ax[1, 0].set_ylabel('Image 1', fontsize=16)\n",
    "ax[1, 1].hist(img2_nz, bins=50);\n",
    "ax[1, 1].set_title('Image 2', fontsize=16)\n",
    "ax[1, 2].scatter(img2_nz, img3_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[1, 2].set_xlabel('Image 2', fontsize=16)\n",
    "ax[1, 2].set_ylabel('Image 3', fontsize=16)\n",
    "ax[1, 3].scatter(img2_nz, img4_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[1, 3].set_xlabel('Image 2', fontsize=16)\n",
    "ax[1, 3].set_ylabel('Image 4', fontsize=16)\n",
    "\n",
    "ax[2, 0].scatter(img3_nz, img1_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[2, 0].set_xlabel('Image 3', fontsize=16)\n",
    "ax[2, 0].set_ylabel('Image 1', fontsize=16)\n",
    "ax[2, 1].scatter(img3_nz, img2_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[2, 1].set_xlabel('Image 3', fontsize=16)\n",
    "ax[2, 1].set_ylabel('Image 2', fontsize=16)\n",
    "ax[2, 2].hist(img3_nz, bins=50);\n",
    "ax[2, 2].set_title('Image 3', fontsize=16)\n",
    "ax[2, 3].scatter(img3_nz, img4_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[2, 3].set_xlabel('Image 3', fontsize=16)\n",
    "ax[2, 3].set_ylabel('Image 4', fontsize=16)\n",
    "\n",
    "ax[3, 0].scatter(img4_nz, img1_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[3, 0].set_xlabel('Image 4', fontsize=16)\n",
    "ax[3, 0].set_ylabel('Image 1', fontsize=16)\n",
    "ax[3, 1].scatter(img4_nz, img2_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[3, 1].set_xlabel('Image 4', fontsize=16)\n",
    "ax[3, 1].set_ylabel('Image 2', fontsize=16)\n",
    "ax[3, 2].scatter(img4_nz, img3_nz, c=all_img_n3_labels, cmap='viridis', vmin=0);\n",
    "ax[3, 2].set_xlabel('Image 4', fontsize=16)\n",
    "ax[3, 2].set_ylabel('Image 3', fontsize=16)\n",
    "ax[3, 3].hist(img4_nz, bins=50);\n",
    "ax[3, 3].set_title('Image 4', fontsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c707d",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-26-15.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The colours in the scatter plots above correspond to the labels we extracted using all four images and three clusters. I.e. blue corresponds to healthy brain tissue, green corresponds to CSF and yellow corresponds to the lesion. This figure shows reasonably well how CSF (green) and lesion (yellow) can be clustered. However, it is not as easy to see how the healthy tissue was separated from CSF and lesion tissue. To investigate further, we can plot the above slightly differently, using a 2-dimensional histogram instead of a scatter plot. \n",
    "\n",
    "## \n",
    "A 2-dimensional histogram plots the counts of values in bins for two variables. The results are displayed as a heatmap. An intuitive example with code using the Matplotlib function `hist2d` [is available here](https://matplotlib.org/3.1.1/gallery/scales/power_norm.html#sphx-glr-gallery-scales-power-norm-py).|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, ax = subplots(4, 4, figsize=(20, 20))\n",
    "\n",
    "ax[0, 0].hist(img1_nz, bins=50);\n",
    "ax[0, 1].hist2d(img1_nz, img2_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[0, 2].hist2d(img1_nz, img3_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[0, 3].hist2d(img1_nz, img4_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "\n",
    "ax[1, 0].hist2d(img2_nz, img1_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[1, 1].hist(img2_nz, bins=50);\n",
    "ax[1, 2].hist2d(img2_nz, img3_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[1, 3].hist2d(img2_nz, img4_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "\n",
    "ax[2, 0].hist2d(img3_nz, img1_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[2, 1].hist2d(img3_nz, img2_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[2, 2].hist(img3_nz, bins=50);\n",
    "ax[2, 3].hist2d(img3_nz, img4_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "\n",
    "ax[3, 0].hist2d(img4_nz, img1_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[3, 1].hist2d(img4_nz, img2_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[3, 2].hist2d(img4_nz, img3_nz, bins=100, norm=mcolors.PowerNorm(0.2));\n",
    "ax[3, 3].hist(img4_nz, bins=50);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76f256",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-clustering-rendered-unnamed-chunk-27-17.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Note in the plot, we used a PowerNorm normalisation on the image intensities. This is just to aid with visualisation, and you are welcome to change or completely remove the normalisation.\n",
    "\n",
    "The plots show that there is a bright, high-density region corresponding to the clustered healthy tissue region. This gives us a better idea how the GMM algorithm found the three regions. Healthy tissue has low signal variance in all 4 images. Signal intensity in CSF and the lesion have much higher variance making it possible to distinguish them from healthy tissue. Furthermore, the relative intensities of CSF and lesion tissue are different as shown in the scatter plots, making it possible for the GMM to distinguish between the two.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "#### End of chapter Exercises\n",
    "\n",
    "In this assignment, we ask you to use the same set of images as in the work through example. However, instead of GMM, we want you to try a different clustering method called __KMeans__. The documentation for KMneans is available here: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Some examples of how kmeans clustering can go wrong are shown in [this example code](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e4bc0",
   "metadata": {},
   "source": [
    "Using `KMeans` from 'sklearn.cluster', do the following tasks:\n",
    "\n",
    "1. Investigate different numbers of clusters, similarly to what we did in th work through example.\n",
    "\n",
    "2. Use different combinations of the 4 images to see how the clustering performs in different cases.\n",
    "\n",
    "3. The labelled results using all four images may not look as clean as the ones in the work-through example. Try scaling the images e.g. using the sklearn standard scaler, and combining the scaled images. Do the results change? If yes, explore and comment on why you think scaling may be advantageous in this clustering example.\n",
    "\n",
    "4. Compare the behaviour of `KMeans` to the outcome with `GaussianMixture`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acc2fd",
   "metadata": {},
   "source": [
    "## Please check these solutions only after submitting the assignments.\n",
    "\n",
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb76c0",
   "metadata": {},
   "source": [
    "## Furture Reading\n",
    "\n",
    "If after this lesson you want to deepen your understanding of clustering and, in particular, want to compare the performance of different clustering methods when dealing with images, try the article [Clustering techniques for\n",
    "neuroimaging applications](https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1174). It is paywalled and you will need an institutional access to download.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c6e83",
   "metadata": {},
   "source": [
    "-\n",
    "-\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949a894",
   "metadata": {},
   "source": [
    "[r-markdown]: https://rmarkdown.rstudio.com/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
